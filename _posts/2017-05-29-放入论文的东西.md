---
title: 放入论文的某些东西
date: 2017-05-20 20:00:00
categories: paper reading
---

1. 贝叶斯  从discrete prior 到continuous prior ？　还是只要 continuous prior ?   
1.1 bayesian 和frequentist 的区别要不要说 ？ 这里要不要 也用frequentist 的方法做一遍 ？


2. SRCNN  -- 普通的CNN 要不要提 ？ 要说多详细 ？  数据集要什么 BSD100 ？

3. 测量相似度的PSNR  SSIM MSSIM

4. preceptral less ---li feifei  

5. GAN ?

6. VGG ?

7. ResNEt ?


# Abstract
# １．Intro
这段里说ＶＴ　和　

# 2 Related work
## 2.1 DCGAN 的发展 
- Tonie 的paper 

## 2.2 贝叶斯的发展
- 贝叶斯公式怎么推导
- discrete prior
- continuous prior
- 前面的常数的意义
- flat prior 和informative prior 怎么选
## 2.3 Super Resolution 的发展
- resampling 方法 bicubic bilinear  
- A+ sparse-coding- method （抄SRCNN）

# 3. method
总起段介绍VT 的方法和统计学原理

## 3.1.1 我统一了PT

## 3.1.2 我使用了Beta 方程
- conjugate prior 存在的意义， 为什么VT 问题 可以用beta 方程  
- beta factor 是什么，为什么选30
- 数据量对结果的影响

## 3.2
总起段介绍SR
## 3.2.1 我使用了SRCNN
- CNN的 结构
- 每一层的参数是？
- 给图 keras 生成
- 给表 keras 生成

## 3.2.2 我使用了 RSRCNN  
- CNN的 结构
- 每一层的参数是？
- 给图 keras 生成
- 给表 keras 生成
- 他的优点是快速

# 4. result
## 4.1 Beta 举例子
- 三个例子
  - 能区别
  - 不能区别
  - 一开始不能区别后来能区别了

## 4.2 SR
- normalisation 的方法
- 给结果图 和 传统方法比较
  - OMNIGLOT 的表现
  - BSD100 SET5 SET 100 的表现
- 给numerical comparision 和 传统方法比较
  - SSIM
  - PSNR


# 5. Conclusion
我发明了一个VT 的方法， 在实际使用中， 发现 原图可能会太小， 所以发明了一种让原图放大但不失真的SR 方法， 并且衡量了他们的表现
